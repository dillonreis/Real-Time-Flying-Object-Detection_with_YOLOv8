\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{bbm}
\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Real Time Flying Object Detection: CS 7643}

\author{Dillon Reis*, Jordan Kupec*, Jacqueline Hong*, Ahmad Daoudi*\\
Georgia Institute of Technology\\
{\tt\small dreis7@gatech.edu, jkupec3@gatech.edu, jhong356@gatech.edu, adaoudi3@gatech.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This project presents a model for inference of a wide array of flying objects in real-time. 
   This remains challenging due to large variance of the object scales during inference, object rate of speed, 
   and low variance between a subset of classes. These findings can then be used for reference/further research 
   regarding object detection by remote digital airport towers, unmanned aerial vehicles (UAVs), or any surveillance 
   systems utilizing optical data. To address some of the presented challenges, we utilize the current state of the art 
   single-shot detector, YOLOv8, in an attempt to find the best trade-off between inference speed and mAP. RESULTS TBD
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction/Background/Motivation}
Numerous recent events have demonstrated the malicious use of drones. Over the past few months, there have been reports of assassination attempts via drones with small explosive payloads \cite{SuicideDrone}, delivering drugs into state prisons \cite{PrisonDrugs}, and surveillance of U.S. Border Patrol by smugglers \cite{BorderPatrol}. While research indicates that drone usage is expected to increase exponentially \cite{DroneMarket}, detection technology have yet to provide reliable and accurate results. Drones and mini UAVs present a stealth capability and can avoid detection by most modern radar systems due to their small electromagnetic signature. They are also small, highly maneuverable, and omit low levels of noise. This, along with the ease of access provides a natural incentive for drones to remain an integral part of modern warfare and illegal activities. While methods such as radio and acoustic detection have been proposed as solutions, they are currently known to be inaccurate(SOURCE). This motivates the integration of a visual detector in any such detection system. To lay out the process of tackling this problem, we will focus on the real-world use case of drone surveillance conducted by smugglers at the U.S. border.

After an encounter with about 30 migrants illegally crossing the border into the U.S., a subsequent investigation led to the discovery of footage recorded by the smugglers of the entire encounter. The U.S. Border Patrol already has digital towers that utilize object detection in order to monitor people and motor vehicles \cite{BorderDetection}, but to the best of our knowledge, they do not detect drones. Their technology primarily focuses on finding migrants but lacks the ability to detect their drones. The surrounding conditions of the border consist of large spans of isolated land and compacted collections of plants \cite{BorderDigitalTowers}, making the landscape and background a challenge for drone detection. Additionally, if the object is far away, not only will it be difficult to detect it, but it will also be harder to correctly classify it as the object will convey less signal to the model. Previous research and projects on flying object detection have a predominant focus on UAVs, specifically Drone-vs-Bird. This is not a proper reflection of real-world conditions, as there are many different varieties of flying objects. We plan to integrate more classes such as specific military and commercial aircraft in our training (citation) to let the model see more variance of flying objects and to better understand important features that make up these objects, leading to a more generalized real time flying object detection model. 

The primary objective of this project is to provide generalizable real time flying object detecition model that is ready to use ``out of the box'' for immediate implementation, transfer learning, or further research. We define a generalizable model as one that has good detection and classification performance at higher resolutions while maintaining a reasonable frame rate (1080p : 30-60 frames per second). To maximize our models performance, we use the latest state-of-the-art single-shot detector, YOLOv8. Currently, single-stage detectors are the de-facto architecture choice for fast inference speeds. This choice comes at the expense of exchanging the higher accuracy you would typically expect from a two-state detector. While YOLOv8 is being regarded as the new state-of-the-art (SOURCE), an official paper has yet to be released. This motivates our secondary objective, which is to explain the new architecture and functionality that YOLOv8 has adapted. 

Real-time object detection remains challenging due to variances in object spatial sizes and aspect ratios, inference speed, and noise. This is especially true for our use case, as flying objects can change location, scale, rotation, and trajectory very quickly. This conveys the necessity for fast inference speed and thorough model evaluation between low variance classes, object sizes, rotations, and aspect ratios.
\\
Our model is trained on a dataset comprised of 15,064 images of various flying objects with an 80\% train and 20\% validation split. Each image is labeled with the class number of the object and the coordinates of the edges of the associated bounding box. An image may have more than one object and class, sitting at an average of 1.6 objects per image. The median image ratio is 416x416. The images were pre-processed with auto-orientation, but there were no augmentations applied. The data set represents a long-tailed distribution with the drone, bird, p-airplane, and c-helicopter classes taking up the majority of the data set, suffering from a class imbalance. Published by Roboflow, this dataset was generated in 2022, having been downloaded only 15 times.
%-------------------------------------------------------------------------
%------------------------------------------------------------------------
\section{Approach}

We chose the YOLOv8 architecture under the assumption that it would provide us the highest probability of success given the task. YoloV8 is 
assumed to be the new state-of-the-art due to its higher mAPs and lower inference speed on the COCO dataset\ref{fig:}. However, an official paper has 
yet to be released. It also specifically performs better of aerial objects, which is the scope of this project. We utilized the code repository 
from Ultralytics. We decide to implement transfer learning and initialize our models with pre-trained weights to then begin training on the 
custom data set. These weights are from a model trained on the COCO dataset. Due to only having access to a single NVIDIA RTX 3080 and 3070, 
a greedy model selection/hyper-parameter tuning approach was chosen. We first train a version of the small, medium, and large versions of the 
model with default hyper-parameters for 100 epochs. Then, we decide which model is optimal for our use case given the trade off between inference 
speed and mAP-50-95 on the validation set. After the model size is selected, a greedy hyper-parameter search is conducted with 10 epochs per each 
set of hyper-parameters. Finally, the model with the optimal hyper-parameters trains for 163 epochs to generate the final model.
Mean Average Precision (mAP) is a commonly used evaluation metric for object detection that combines precision, recall, IOU, and AP. The curve we obtain is commonly called the precision-recall curve. Average Precision is therefore defined as the area underneath this curve. A higher AP score indicates better performance since it means the model is accurately detecting more objects in an image while minimizing false positives.
For each class, we divide the recall values into 11 points ranging from 0.5 to 0.95 with step-size 0.05 plot that against maximum precision from that recall value upward. We obtain mAP by averaging over each classes AP \ref{mAP}\\ 

\begin{align}\label{mAP}
AP=\dfrac{1}{11}\sum_{r\in{0.5,...0.95}}AP_r \\*
=\dfrac{1}{11}\sum_{r\in{0.5,...0.95}}p_{interp}{r}
\end{align}
where:
\begin{equation*}
p_{interp}({r}) = \max_{\widetilde{r}\geq{r}} p(\widetilde{r})
\end{equation*}
Due to the large class imbalance, poor performance on the validation set was anticipated on the minority classes. However, this was not observed\ref{fig:confusion_matrix}.

\subsection{Model Choice and Evaluation}

We evaluate small, medium, and large versions of the models to determine an optimal trade off between inference speed and mAP50-95 to then optimize the hyper-parameters. The small, medium, and large models have (11151080, 25879480, \& 43660680) parameters and (225,295, \& 365) layers respectively. After training the models we see there is a noticeable increase in mAP50-95 between small and medium models (0.05), but not much delta between medium and large (0.002). We also see that small, medium, and large infer at 4.1,5.7, and 9.3 milliseconds respectively on the validation set containing 640 x 640 images. However, our original goal is to reach an average inference speed between 30 to 60 frames for 1080p. When testing the medium size model on multiple 1080p HD videos, we observe an average total speed (pre-proccess speed(0.5ms) + inference speed(17.25ms) + post-process speed(2ms)) of 19.75 ms (50 frames per second), which aligns with our primary objective. This leads to our selection of the medium size model to begin tuning hyper-parameters.

Due to a lack of computational resources, we evaluate only 10 epochs for each small set of hyper-parameters as an indicator for the potential performance of additional epochs. We observe that this assumption is correct, as training with the optimal set of hyper parameters achieves better performance at epoch 100 compared to default hyper-parameters (0.027)\ref{fig:YOLOv8_mAP50-95_val}. This takes around 35 minutes per every 10 epochs. After evaluating mAP50-95 on the validation set for 10 epochs, we choose batch size of 16, stochastic gradient descent (SGD) as the optimizer, momentum of 0.937, weight decay of 0.01, classification loss weight of 1, box loss weight of 5.5, and distribution focal loss weight of 2.5. After training for 163 epochs we achieve an mAP50-95 of 0.685 and recall of 0.792.

\subsection{Loss Function and Update Rule}
The generalized loss function and weight update procedure can be defined as follows: 
\begin{equation}\label{Generalized Loss}
\mathcal{L}(\theta) = \dfrac{\lambda_{box}}{N_{pos}}\mathcal{L}_{box}(\theta) + \dfrac{\lambda_{cls}}{N_{pos}}\mathcal{L}_{cls}(\theta) + \dfrac{\lambda_{dfl}}{N_{pos}}\mathcal{L}_{dfl}(\theta) + \phi\Vert \theta \Vert_2^2
\end{equation}    
\begin{equation}\label{Velocity}
V^t = \beta V^{t-1} + \nabla_{\theta}\mathcal{L}(\theta^{t-1})
\end{equation}    
\begin{equation}\label{Weight Update}
\theta^{t} = \theta^{t-1} - \eta V^{t}
\end{equation}

Where \ref{Generalized Loss} is the generalized loss function incorporating the individual loss weights and a regularization term with weight decay $\phi$, \ref{Velocity} is the velocity term which takes momentum $\beta$ as a parameter, and \ref{Weight Update} which is the weight update rule. The specific YoloV8 loss function can be defined as:
\begin{multline}
\mathcal{L} = \dfrac{\lambda_{box}}{N_{pos}}\sum_{x,y}\mathbbm{1}_{c^*_{x,y}}\big[1 - q_{x,y} + \dfrac{\Vert b_{x,y} - \hat{b}_{x,y} \Vert_2^2}{c^2} + \alpha_{x,y}\nu_{x,y}\big] \\  + \dfrac{\lambda_{cls}}{N_{pos}}\sum_{x,y}\sum_{c\in classes}y_clog(\hat{y}_c) + (1 - y_c)log(1 - \hat{y}_c) \\
+ \dfrac{\lambda_{dfl}}{N_{pos}}\sum_{x,y}\mathbbm{1}_{c^*_{x,y}}\Big[-(q_{(x,y)+1} - q_{x,y})log(\hat{q}_{x,y}) \\
+ (q_{x,y} - q_{(x,y)-1})log(\hat{q}_{(x,y)+1})\Big]
\end{multline}

where:
\begin{equation*}\label{IoU}
q_{x,y} = IoU_{x,y} = \dfrac{\hat{\beta}_{x,y}\displaystyle \cap\beta_{x,y}}{\hat{\beta}_{x,y}\displaystyle \cup\beta_{x,y}}
\end{equation*}
\begin{equation*}\label{v}
\nu_{x,y} = \dfrac{4}{\pi^2}(arctan(\dfrac{w_{x,y}}{h_{x,y}}) - arctan(\dfrac{\hat{w}_{x,y}}{\hat{h}_{x,y}}))^2
\end{equation*}
\begin{equation*}\label{a}
\alpha_{x,y} = \dfrac{\nu}{1 - q_{x,y}}
\end{equation*}
\begin{equation*}\label{y_hat}
\hat{y}_c = \sigma({\cdot})
\end{equation*}
\begin{equation*}\label{q_hat}
\hat{q}_{x,y} = softmax({\cdot})
\end{equation*}
    
    
and:
\begin{itemize}[noitemsep]
\item $N_{pos}$ is the total number of cells containing an object.
\item $\mathbbm{1}_{c^*_{x,y}}$ is an indicator function for the cells containing an object. 
\item $\beta_{x,y}$ is a tuple that represents the ground truth bounding box consisting of ($x_{coord}$,$y_{coord}$,width,height).
\item $\hat{\beta_{x,y}}$ is the respective cell predicted box.
\item $b_{x,y}$ is a tuple that represents the central point of ground truth bounding box.
\item $q_{(x,y)+/- 1}$ are the nearest predicted boxes IoUs (left and right) $\in c^*_{x,y}$
\item $w_{x,y}$ and $h_{x,y}$ are the respective boxes width and height.
\item c is the diagonal length of the smallest enclosing box covering the predicted and ground truth boxes.
\end{itemize}

Each cell then determines its best candidate for predicting the bounding box of the object. This loss function includes the complete IoU (https://arxiv.org/pdf/1911.08287.pdf) loss as the box loss, the standard binary cross entropy for multi-label classification as the classification loss (allowing each cell to predict more than 1 class), and the distribution focal loss in the 3rd term (https://arxiv.org/abs/2006.04388).
    
\begin{figure*}[h]
    \centering
    \subfloat[\centering Confusion matrix for all classes \label{fig:confusion_matrix}]{{\includegraphics[width=9.5cm,height=7cm]{figures/confusion_matrix.png}}}
    \qquad
    \subfloat[\centering YOLOv8 validation mAP50-95 \label{fig:YOLOv8_mAP50-95_val}]{{\includegraphics[width=7cm,height=7.9cm]{figures/Pre-Trained YOLOv8 Val mAP50-95.png} }}%
    \caption{YOLOv8 Validation}%
    \label{fig:Model_Evaluation}
\end{figure*}

\section{Experiments and Results}
\subsection{Low Intraclass Variance Discussion}
Classes that are similar to each other in an object detection algorithm can pose a challenge for the model as it may struggle to distinguish between the two classes. For example, if the dataset we use contains many different families of planes which the model may have difficulty distinguishing between them. This problem is often called class confusion and can typically effect models object detection models.

To combat this problem, one approach is to view a confusion matrix, which visualizes the number of predicted classes versus the ground truth label. Looking at our confusion matrix \ref{fig:confusion_matrix}, the model does surprisingly well for low intraclass variance categories, such as the "fighter" family. 

%-------------------------------------------------------------------------

\section{Model Architecture}
With the publication of “You Only Look Once: Unified, Real-Time Object Detection” \cite{YOLO_OG} in 2015, one of the most popular object detection algorithms, YOLOv1, was first described as having a “refreshingly simple” approach \cite{CompReview}. At its inception, YOLOv1 could process images at 45 fps, while a variant, fast YOLO, could reach upwards of 155 fps. It also achieved high mAP compared to other object detection algorithms at the time.\\
\indent The main idea of YOLO is to frame the problem of object detection as a one-pass regression problem, YOLOv1 comprises a single neural network, predicting bounding boxes and associated class probability in a single evaluation. The base model of YOLO works by first dividing the input image into an S x S grid where each grid cell (i,j) predicts B bounding boxes, a confidence score for each box and C class probabilities. The final output will be a tensor of shape: S x S x (B x 5 + C).

\subsection{YOLOv1 Overview}
YOLOv1 architecture \ref{fig:YOLOv1Architecture} consists of 24 convolutional layers followed by two fully connected layers. In the paper, the authors took the first 20 convolutional layers from the backbone of the network and, with the addition of an average pooling layer and a single fully connected layer, where it was pre-trained and validated on the ImageNet 2012 dataset. During inference, the final four layers and 2 FC layers are added to the network; all initialized randomly.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/YOLOv1 Architecture.png}
    \caption{YOLO Architecture \cite{YOLO_OG}}
    \label{fig:YOLOv1Architecture}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.4\textwidth]{figures/YOLO_Loss.png}
%     \caption{YOLOv1 Loss function ~\cite{YOLO_OG}}
%     \label{fig:YOLO_Loss}
% \end{figure}\\

YOLOv1 uses stochastic gradient descent as its optimizer; the Loss function is shown here \ref{YOLO_Loss_equation}. The Loss function \ref{YOLO_Loss_equation} comprises two parts, localization loss, and classification loss. The localization loss measures the error between the predicted bounding box coordinates and the ground-truth bounding box. The classification loss measures the error between the predicted class probabilities and the ground truth. The $\lambda_{coord}$ and $\lambda_{noobj}$ are regularization coefficients that regulate the magnitude of the different components, emphasizing object localization and deemphasizing grid cells without objects. 

\begin{multline} \label{YOLO_Loss_equation}
\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbbm{1}_{ij}^{obj}\Big[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2\Big] \\ + \lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbbm{1}_{ij}^{obj}\Big[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2\Big]\\
+\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbbm{1}_{ij}^{obj}(C_i-\hat{C}i)^2\\
+\lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbbm{1}_{ij}^{noobj}(C_i-\hat{C}i)^2\\
+\sum_{i=0}^{S^2}\mathbbm{1}_{i}^{obj}\sum_{c\in classes}(p_i(c)-\hat{p}_i(c))^2
\end{multline}
\subsection{YOLOv5 Overview}
YOLOv5 is an object detection model that was first introduced in 2020 by Ultralytics, which builds on the success of previous YOLO models. YOLOv5 achieved SOTA performance on a variety of benchmark datasets while also being fast and efficient to train and deploy. YOLO made several architectural changes when YOLOv5 came around, most notably the standardized practice of structuring the model into three components, the backbone, the neck, and the head.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Yolov5Architecture.png}
    \caption{YOLOv5 Architecture \cite{Yolov5Arch}}
    \label{fig:Yolov5Architecture}
\end{figure}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0 \textwidth]{figures/YOLOv8_arch.png}
    \caption{YOLOv8 Architecture ~\cite{YOLOv8Website}}
    \label{fig:YOLOv8_arch}
\end{figure*}
\begin{figure}[h]
    \centering
    \subfloat[\centering YOLOs mAP@.50 against RF100 ~\cite{YOLOv8Website} \label{fig:YOLOv8_mAP}]{{\includegraphics[width=3cm]{figures/YOLOv5_vs_YOLOv8_COMP1.png}}}
    \qquad
    \subfloat[\centering YOLOs average mAP@.50 against RF100 categories \label{fig:YOLOv8_average_mAP_against_cats}]{{\includegraphics[width=3cm]{figures/YOLOv5_vs_YOLOv8_COMP2.png} }}%
    \caption{YOLOv8 vs Previous Versions}%
    \label{fig:Model_Evaluation}
\end{figure}

The backbone of YOLOv5 is a variant of Darknet53, a new network for performing feature extraction characterized by small filter windows and residual connections. YOLOv5 modifies Darknet 53 by introducing Cross Stage Partial (CSP) connections to the network. This enables the architecture to achieve a richer gradient combination while reducing the amount of computation. It involves dividing the backbone into two streams, one processing the image and another which processes a downsampled version of the input. They are later concatenated and passed to the next stage of the network. \\
The neck of YOLOv5 is the Intermediate component that connects the backbone to the head. It aggregates and refines the features extracted by the backbone, often focusing on enhancing the spatial and semantic information across different scales.\\
The first half of the neck is composed of a Spatial Pyramid Pooling (SPP) layer to remove the fixed-size constraint of the network, which is typically a requirement due to the final fully connected layers during the inference phase. This removes the need to warp, augment, or crop images, which generally change the aspect ratio and scale of the object in the image. 

The second half of the neck, the CSP-Path Aggregation Network (CSP-PAN), incorporates the features learned by the CSP blocks in the backbone and shortens the information path between lower layers and top features. PAN links the feature grid and all feature levels to make helpful information in each feature level propagate directly to subblocks.

YOLOv5’s head consists of three branches, each predicting a different feature scale. In the original publication, the authors used different grid cell shapes for each level, especially grid cell sizes of 13 x 13, 26 x 26, and 52 x 52, with each grid cell predicting B = 3 bounding boxes. Each head produces bounding boxes, class probabilities, and confidence scores, with the output tensor being of shape S x S x (B x 5 + C), where 5 represents x, y, w, h, and the confidence score of the predicted class. Finally, the network uses Non-maximum Suppression (NMS) to filter out overlapping bounding boxes and only keep the most confident predictions.

YOLOv5 also incorporates anchor boxes, fixed-sized bounding boxes used to predict the location and size of objects within an image. Instead of predicting arbitrary bounding boxes for each object instance, the model predicts the coordinates of the anchor boxes with predefined aspect ratios and scales and adjusts them to fit the object instance.

\subsection{YOLOv8 Overview}
YOLOv8 is the latest version of the YOLO object detection model. This latest version has the same architecture as its predecessors \ref{fig:YOLOv8_arch} but it introduces numerous improvements compared to the earlier versions of YOLO such as a new neural network architecture that utilizes both Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) and a new labeling tool that simplifies the annotation process. This labeling tool contains several useful features like auto labeling, labeling shortcuts, and customizable hotkeys. The combination of these features makes it easier to annotate images for training the model.

The FPN works by gradually reducing the spatial resolution of the input image while increasing the number of feature channels. This results in the creation of feature maps that are capable of detecting objects at different scales and resolutions. The PAN architecture, on the other hand, aggregates features from different levels of the network through skip connections. By doing so, the network can better capture features at multiple scales and resolutions, which is crucial for accurately detecting objects of different sizes and shapes ~\cite{CompReview}

\subsection{YOLOv8 vs YOLOv5}
The reason YOLOv8 is being compared to YOLOv5 and not any other version of YOLO is that YOLOv5’s performance and metrics are closer to YOLOv8’s. However, YOLOv8 surpasses YOLOv5 in aspects including a better mAP as seen in Figure \ref{fig:YOLOv8_mAP}. Along with a better mAP, this shows that YOLOv8 has fewer outliers when measured against the RF100 which is a 100-sample dataset from the Roboflow universe which is a repository of 100,000 datasets. We also witness YOLOv8 outperforming YOLOv5 for each RF100 category. From the Figure \ref{fig:YOLOv8_average_mAP_against_cats} we can see that YOLOv8 produces similar or even better results compared to YOLOv5 ~\cite{YOLOv8Website}.

As mentioned previously, YOLOv8 uses a new architecture that combines both FAN and PAN modules. FPN is used to generate feature maps at multiple scales and resolutions, while PAN is used to aggregate features from different levels of the network to improve accuracy. The results of the combined FAN and PAN modules are better than YOLOv5 which uses a modified version of CSPDarknet architecture. This modified version of CSPDarknet is based off the cross-stage partial connections (CSP), which improves the flow of information between different parts of the network.

Another difference the two models have is their training data. YOLOv8 was trained on a larger and more diverse dataset compared to YOLOv5. YOLOv8 was trained on a blend of the COCO dataset and several other datasets, while YOLOv5 was trained primarily on the COCO dataset. Because of that, YOLOv8 has a better performance on a wider range of images.

YOLOv8 includes a new labeling tool called RoboFlow Annotate which is used for image annotation and object detection tasks in computer vision. RoboFlow Annotate makes it easier to annotate images for training the model and includes several features such as auto labeling, labeling shortcuts, and customizable hotkeys. In contrast, YOLOv5 uses a different labeling tool called LabelImg. LabelImg is an open-source graphical image annotation tool that allows its users to draw bounding boxes around objects of interest in an image, and then export the annotations in the YOLO format for training the model.

YOLOv8 includes more advanced post-processing techniques than YOLOv5, which are a set of algorithms applied to the predicted bounding boxes and objectiveness scores generated by the neural network. These techniques help to refine the detection results, remove redundant detections, and improve the overall accuracy of the predictions. YOLOv8 uses Soft-NMS which is a variant of the NMS technique used in YOLOv5. Soft-NMS applies a soft threshold to the overlapping bounding boxes instead of discarding them outright. Whereas NMS removes the overlapping bounding boxes and keeps only the ones with the highest objectiveness score.

Output heads refer to the final layers of a neural network that predict the locations and classes of objects in an image. In YOLO architecture there are typically several output heads that are responsible for predicting different aspects of the detected objects, such as the bounding box coordinates, class probabilities, and objectiveness scores. These output heads are typically connected to the last few layers of the neural network and are trained to output a set of values that can be used to localize and classify objects in an image. The number and type of output heads used can vary depending on the specific object detection algorithm and the requirements of the task at hand. YOLOv5 has 3 output heads while YOLOv8 has 1 output head. YOLOv8 Does not have small, medium, and large anchor boxes rather it uses an anchor free detection mechanism that directly predicts the center of an object instead of the offset from a known anchor box which reduces the number of box predictions, and that speeds up the post processing process.

It is fair to note that YOLOv8 is slightly slower than YOLOv5 when talking about object detection speed. However, YOLOv8 is still able to process images in real-time on modern GPUs.

Both YOLOv5 and YOLOv8 use mosaic augmentation on the training set. Mosaic augmentation is a data augmentation technique that takes four random images from the training set and combines them into a single mosaic image. This image, where each quadrant contains a random crop from one of the four input images, is then inputted into the model ~\cite{MosaicAug}
%---------------------------------------------------------------------

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/yolo-comparison-plots.png}
%     \caption{Shows the performance of each YOLO model and its respective versions for mAP50 against a number of parameters and inference speed}
%     \label{fig:my_label}
% \end{figure}

\newpage
\newpage

														 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\begin{table*}
\begin{center}			  
\begin{tabular}{|l|c|p{8cm}|}
\hline
Student Name & Contributed Aspects & Details \\
\hline\hline
Team Member 1 & Data Creation and Implementation & Scraped the dataset for this project and trained the CNN of the encoder. Implemented attention mechanism to improve results. \\
Team Member 2 & Implementation and Analysis & Trained the LSTM of the encoder and analyzed the results. Analyzed effect of number of nodes in hidden state.  Implemented Convolutional LSTM. \\
Team Member 3 & Implementation and Analysis & Trained the LSTM of the encoder and analyzed the results. Analyzed effect of number of nodes in hidden state.  Implemented Convolutional LSTM. \\
Team Member 4 & Implementation and Analysis & Trained the LSTM of the encoder and analyzed the results. Analyzed effect of number of nodes in hidden state.  Implemented Convolutional LSTM. \\
\hline
\end{tabular}
\end{center}
\caption{Contributions of team members.}
\label{tab:contributions}
\end{table*}
\end{document}